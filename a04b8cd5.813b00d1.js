(window.webpackJsonp=window.webpackJsonp||[]).push([[7],{120:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/unwrap_multiple_fragments-b3ea49b8c2258d6e956635fcbf0b9b7e.jpg"},121:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/match_frames-ef3cace18ed62b0831d3debfc11a084b.jpg"},122:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/left_fragment_keypoints-ce68423fbe7149b1927071909e9d559d.jpg"},123:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/right_fragment_keypoints-628dbab2572dcc6807793d3b56c17802.jpg"},124:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/fragment_matching_sift-63f77e5eb8e0d4f01f7ab9548cb49a61.jpg"},125:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/fragment_matching_sift_corrected-62c2c2db420b9e7f77d07f3af27e3138.jpg"},126:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/left_fragment_transformed-fa3099ac7d7f55012f659789fec1f6b3.jpg"},127:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/right_fragment_transformed-4d385c2902913d7e43a5feb5d82c99f5.jpg"},128:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/stitched_without_optical_flow-df6bd3cfc5a3fc88c2fc361896c9158e.jpg"},129:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/stitched_animation-65e8b76a45f88d0d20660ffd7290c9c1.gif"},130:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/optical_flow-8483552ed7cce4ad5dfaad0d3c5e7299.png"},131:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/optical_flow_split-db9fceb7c34b2769958b369f4e3ee9ed.png"},132:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/optical_flow_animation-2249a063a92d81db9379ff73b786cc72.gif"},133:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/stitched_with_optical_flow-b1c212c5b9931b8f80f58f934faf3efb.jpg"},134:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/blending_animation-3980b6cf8c229fb24afda617ac7d9e8a.gif"},135:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/stitched_with_blend-5b46d2815e11dc4fee7b217f263c9828.jpg"},136:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/frames_set-cabda4a843f755e0784576c4f456c28d.jpg"},137:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/stitched_long-2a00c2e9fb4014ec38b3d566b0feeb74.jpg"},67:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return o})),a.d(t,"metadata",(function(){return r})),a.d(t,"rightToc",(function(){return l})),a.d(t,"default",(function(){return f}));var i=a(2),n=a(6),s=(a(0),a(85)),o={slug:"seamless-stitching",title:"Seamless Stitching of Perfect Labels",author:"Alexey Zankevich",authorURL:"https://github.com/Nepherhotep/",tags:["stitching","perfectlabel"]},r={permalink:"/blog/seamless-stitching",editUrl:"https://github.com/robopickles/blog/2020-08-27-seamless-stitching-of-perfect-labels.mdx",source:"@site/blog/2020-08-27-seamless-stitching-of-perfect-labels.mdx",description:"The previous articles describe how to unwrap labels programmatically and how we",date:"2020-08-27T00:00:00.000Z",tags:[{label:"stitching",permalink:"/blog/tags/stitching"},{label:"perfectlabel",permalink:"/blog/tags/perfectlabel"}],title:"Seamless Stitching of Perfect Labels",readingTime:4.295,truncated:!0},l=[],c={rightToc:l};function f(e){var t=e.components,o=Object(n.a)(e,["components"]);return Object(s.b)("wrapper",Object(i.a)({},c,o,{components:t,mdxType:"MDXLayout"}),Object(s.b)("p",null,"The previous articles describe how to unwrap labels programmatically and how we\ntrained a neural network to detect keypoints.\nThis article is about how to stitch multiple images into a single long one."),Object(s.b)("p",null,Object(s.b)("img",{alt:"logo",src:a(96).default})),Object(s.b)("p",null,"The original set of images (below) was segmented and unwrapped in advance by a\nneural network, as described in the previous articles. Please see them for\nmore detail."),Object(s.b)("p",null,Object(s.b)("img",{alt:"multiple unwrap",src:a(120).default})),Object(s.b)("p",null,"How does stitching even work? We take two images with overlapping areas,\ncompute the mutual shift from each other, and blend them. Sounds pretty easy,\nbut let\u2019s go through each of the steps.\nTo calculate the shift, it\u2019s required to find something that exists on both\nimages, and find a formula to convert points from the first image into points\non the second one. The mentioned shift can be represented by a homography\nmatrix, where the cell values encode the different types of transformations\ntogether \u2014 scaling, translation, and rotation.\nAs we can see, in these photos, there are plenty of common objects:"),Object(s.b)("p",null,Object(s.b)("img",{alt:"match frames",src:a(121).default})),Object(s.b)("p",null,"The problem with the given features is that it\u2019s hard to detect them\nprogrammatically. Luckily, there are algorithms that detect \u201cgood\u201d\nfeatures (also known as \u201ccorners\u201d \u2014 there is an excellent document\nthat explains them).\nOne such algorithm is SIFT (Scale-Invariant Feature Transform).\nDespite being invented in 1999, it\u2019s still very popular due to its simplicity\nand reliability. Since SIFT is patented, it\u2019s a part of OpenCV non-free build,\nbut the patent recently expired (in March 2020), so it will probably become a\npart of standard OpenCV soon.\nNow, let\u2019s find similar features on both images:"),Object(s.b)("pre",null,Object(s.b)("code",Object(i.a)({parentName:"pre"},{}),"sift = cv2.xfeatures2d.SIFT_create()\nfeatures_left = sift.detectAndCompute(left_image, None)\n")),Object(s.b)("p",null,Object(s.b)("img",{alt:"left image keypoints",src:a(122).default})),Object(s.b)("pre",null,Object(s.b)("code",Object(i.a)({parentName:"pre"},{}),"features_right = sift.detectAndCompute(right_image, None)\n")),Object(s.b)("p",null,Object(s.b)("img",{alt:"right image keypoints",src:a(123).default})),Object(s.b)("p",null,"Using a Flann based matcher can find matches relatively quickly between two\nimages, despite a large number of corners.\nThe yellow lines in the picture below connect similar features in the left\nand right images."),Object(s.b)("p",null,Object(s.b)("img",{alt:"fragment matching sift",src:a(124).default})),Object(s.b)("p",null,"For clarity, the images were placed above each other with the proper offset\n(which is calculated at a later point in the process, rather than here).\nAs it\u2019s easy to see, there are still lots of mismatches \u2014 about 50% of them\nwere wrong. However, the good matches will always generate the same\ntranslation, meantime the bad ones will give chaotically different directions.\nA picture below depicts only the good matches:"),Object(s.b)("p",null,Object(s.b)("img",{alt:"fragment matching sift corrected",src:a(125).default})),Object(s.b)("p",null,"One of the approaches to find a proper translation is a RANSAC algorithm.\nIt goes through the matches iteratively and uses a voting approach to figure\nout if the proper translation is already found.\nLuckily, OpenCV provides multiple choices to find homography using RANSAC \u2014\nthe difference between methods is in dimensions of freedom the transformation\nis going to have. In our case, we need to use estimateAffinePartial2D which\nwill look for the following transformations: rotation + scaling + translation\n(4 dimensions of freedom)."),Object(s.b)("pre",null,Object(s.b)("code",Object(i.a)({parentName:"pre"},{}),"H, _ = cv2.estimateAffinePartial2D(right_matches, left_matches, False)\n")),Object(s.b)("p",null,"The left image:\n",Object(s.b)("img",{alt:"left fragment transformed",src:a(126).default})),Object(s.b)("p",null,"The right image:\n",Object(s.b)("img",{alt:"right fragment transformed",src:a(127).default})),Object(s.b)("p",null,"Let\u2019s blend them using a naive method, where the intersected region is\ncalculated as a mean of left and right images. Unfortunately, the result isn\u2019t\nreally impressive \u2014 there are double vision artifacts across the image,\nespecially closer to stitches.\n",Object(s.b)("img",{alt:"stitched without optical flow",src:a(128).default})),Object(s.b)("p",null,"The animation shows the shift between the tiles:\n",Object(s.b)("img",{alt:"animation stitched",src:a(129).default})),Object(s.b)("p",null,"It\u2019s not surprising \u2014 the images were taken from slightly different angles,\nand there are tiny differences between them.\nFor seamless stitching, it\u2019s required to compensate for those non-linear\ndistortions. The distortion can be described as a smooth vector field, with\nthe same resolution as the original tiles. This vector field is called\n\u201coptical flow\u201d.\n",Object(s.b)("img",{alt:"optical flow",src:a(130).default})),Object(s.b)("p",null,"There are several techniques to calculate the flow \u2014 with functions that come\nwith OpenCV or special neural network architectures.\nOur optical flow for the given two tiles:\nTo avoid artifacts during stitching, it\u2019s required to compensate both images\nproportionally. For that purpose, we split the optical flow into two matrices:\n",Object(s.b)("img",{alt:"optical flow split",src:a(131).default})),Object(s.b)("p",null,"Now the two images are almost perfectly aligned:\n",Object(s.b)("img",{alt:"animation optical flow",src:a(132).default})),Object(s.b)("p",null,"Once we blend the full image, it appears to be geometrically correct, but we\nobserve a brightness jump:\n",Object(s.b)("img",{alt:"stitched with optical flow",src:a(133).default})),Object(s.b)("p",null,"The issue is quite easy to fix if, instead of mean values, we use a blending\nformula, where the values are applied with the gradient:\n",Object(s.b)("img",{alt:"blending animation",src:a(134).default})),Object(s.b)("p",null,"With that approach, the stitching is absolutely seamless:\n",Object(s.b)("img",{alt:"stitched with blending",src:a(135).default})),Object(s.b)("p",null,"There are other blending algorithms that work well with panorama stitching\n(such as multiband blending), but they don\u2019t work well for images with text \u2014\nonly optical flow compensation can completely remove ghosting on characters."),Object(s.b)("p",null,"Let\u2019s stitch the entire set of images:\n",Object(s.b)("img",{alt:"frames set",src:a(136).default})),Object(s.b)("p",null,"The final result:\n",Object(s.b)("img",{alt:"stitched long",src:a(137).default})),Object(s.b)("p",null,"Future improvements could be a shadow effect compensation (the right side of\nthe image), and even doing more post-processing to improve colors and contrast.\nHere we learned how stitching works, and how to make it seamless for production\nuse. The service is available for everyone at Perfect Label as a REST API."),Object(s.b)("p",null,"The recommended links:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"SIFT explained ",Object(s.b)("a",Object(i.a)({parentName:"li"},{href:"https://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html"}),"https://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html")),Object(s.b)("li",{parentName:"ul"},"OpenCV homography explained ",Object(s.b)("a",Object(i.a)({parentName:"li"},{href:"https://docs.opencv.org/master/d9/dab/tutorial_homography.html"}),"https://docs.opencv.org/master/d9/dab/tutorial_homography.html")),Object(s.b)("li",{parentName:"ul"},"Panorama Autostitching ",Object(s.b)("a",Object(i.a)({parentName:"li"},{href:"http://matthewalunbrown.com/papers/ijcv2007.pdf"}),"http://matthewalunbrown.com/papers/ijcv2007.pdf")),Object(s.b)("li",{parentName:"ul"},"OpenPano ",Object(s.b)("a",Object(i.a)({parentName:"li"},{href:"https://github.com/ppwwyyxx/OpenPano"}),"https://github.com/ppwwyyxx/OpenPano")),Object(s.b)("li",{parentName:"ul"},"Google Photo Scanner ",Object(s.b)("a",Object(i.a)({parentName:"li"},{href:"https://ai.googleblog.com/2017/04/photoscan-taking-glare-free-pictures-of.html"}),"https://ai.googleblog.com/2017/04/photoscan-taking-glare-free-pictures-of.html"))))}f.isMDXComponent=!0},96:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/logo-6888d86f696f3c4d88682f080b31d701.jpg"}}]);